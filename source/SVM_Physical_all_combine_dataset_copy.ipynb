{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os\n",
    "import numpy\n",
    "import plotly.express\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, balanced_accuracy_score, matthews_corrcoef, average_precision_score\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Créez un DataFrame vide\n",
    "df = []\n",
    "\n",
    "# Chemin vers le répertoire contenant vos fichiers CSV\n",
    "csv_files = glob.glob('data/Physical dataset/*.csv')\n",
    "print(csv_files)\n",
    "# Parcourez la liste de fichiers CSV et ajoutez-les au DataFrame\n",
    "for csv_file in csv_files:\n",
    "    delimiter = '\\t'\n",
    "    encoding=\"utf-16le\"\n",
    "    if csv_file.endswith(\"4.csv\"):\n",
    "        delimiter = ','\n",
    "        encoding = \"utf-8\"\n",
    "    df_temp = pandas.read_csv(csv_file, encoding=encoding, delimiter=delimiter)\n",
    "    df.append(df_temp)\n",
    "\n",
    "df  = pandas.concat(df, ignore_index=True)\n",
    "# Maintenant, df contient l'ensemble des données de tous les fichiers CSV\n",
    "DOWN=1 #downsampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Remplacer les valeurs non nulles dans Label_n par celles de Lable_n\n",
    "df['Label_n'] = df['Label_n'].combine_first(df['Lable_n'])\n",
    "\n",
    "# Supprimer la colonne Lable_n\n",
    "df.drop('Lable_n', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame X you will use for cleaning.\n",
    "X = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ancien=X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsampling\n",
    "\n",
    "if DOWN:\n",
    "    df_class_0 = X[X['Label_n'] == 0]\n",
    "    df_class_1 = X[X['Label_n'] == 1]\n",
    "    min_rows = min(df_class_0.shape[0], df_class_1.shape[0])\n",
    "\n",
    "    df_class_0_sampled = df_class_0.sample(n=min_rows, random_state=42)\n",
    "    df_balanced = pandas.concat([df_class_0_sampled, df_class_1], ignore_index=True)\n",
    "\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(df_balanced.shape)\n",
    "    X=df_balanced\n",
    "\n",
    "print(\"nombre de fraude et non-fraude\\n\",X['Label_n'].value_counts() )\n",
    "prc=100*df['Label_n'].value_counts()[1]/(X['Label_n'].value_counts()[1]+X['Label_n'].value_counts()[0])\n",
    "print(\"pourcentage de fraude:\",prc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the attack_cat labels in a DataFrame Y as a 1-D array\n",
    "Y = X['Label_n'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on drop Label, Time et Label_n pour pouvoir faire l'apprentissage\n",
    "#Label_n met aussi les erreur physique comme attaque, or ce ne sont pas des attaques. C'est pourquoi on utilise pas cette colonne et on créer nous même une colonne.\n",
    "X.drop(['Label', 'Time','Label_n'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract train and test data through train_test_split on X and Y\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez les bibliothèques nécessaires\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Créez le modèle SVM\n",
    "svm_model = SVC(kernel='poly')  # Vous pouvez choisir différents noyaux comme 'linear', 'poly', 'rbf', etc.\n",
    "\n",
    "# Entraînez le modèle sur l'ensemble d'entraînement\n",
    "svm_model.fit(X_train, Y_train)\n",
    "\n",
    "# Faites des prédictions sur l'ensemble de test\n",
    "Y_pred = svm_model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Confusion matrix:\", confusion_matrix(Y_test, Y_pred))\n",
    "print(\"Precision:\", precision_score(Y_test, Y_pred))\n",
    "print(\"Recall:\", recall_score(Y_test, Y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(Y_test, Y_pred))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(Y_test, Y_pred))\n",
    "print(\"AUPRC:\", average_precision_score(Y_test, Y_pred))\n",
    "\n",
    "fig = plotly.express.imshow(confusion_matrix(Y_test, Y_pred), title=\"Confusion matrix\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x         = [1,2,3,4,5]\n",
    "precision = [0.782312925170068  ,0.7916666666666666 ,0.875               , 0.7578947368421053 ,0.7370242214532872]\n",
    "Recall    = [0.8914728682170543 ,0.8636363636363636 ,0.8235294117647058  ,1.0                 ,0.852]\n",
    "Accuracy  = [0.8277153558052435  ,0.8333333333333334, 0.8245614035087719 ,0.8270676691729323  ,0.7757936507936508]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(x,precision, label='Precision')\n",
    "plt.plot(x,Recall, label='Recall')\n",
    "plt.plot(x,Accuracy, label='Accuracy')\n",
    "\n",
    "plt.xlabel('file attack')\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "\n",
    "plt.legend()\n",
    "plt.title('SVM physical')\n",
    "\n",
    "plt.savefig('physical_precision_recall_accuracy.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sur l'ensemble des données\n",
    "X_test=X_ancien\n",
    "Y_test = X_test['Label_n'].copy()\n",
    "X_test.drop(['Label', 'Time','Label_n'], axis=1, inplace=True)\n",
    " \n",
    " \n",
    "# Faites des prédictions sur l'ensemble de test\n",
    "Y_pred = svm_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Confusion matrix:\", confusion_matrix(Y_test, Y_pred))\n",
    "print(\"Precision:\", precision_score(Y_test, Y_pred))\n",
    "print(\"Recall:\", recall_score(Y_test, Y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(Y_test, Y_pred))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(Y_test, Y_pred))\n",
    "print(\"AUPRC:\", average_precision_score(Y_test, Y_pred))\n",
    "\n",
    "fig = plotly.express.imshow(confusion_matrix(Y_test, Y_pred), title=\"Confusion matrix\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x         = [1,2,3,4,5]\n",
    "precision = [0.547945205479452  ,0.18666666666666668 ,0.29878048780487804 , 0.41362916006339145,0.19138276553106212]\n",
    "Recall    = [0.900562851782364 ,0.875 ,0.8596491228070176  ,0.9849056603773585                 ,0.9474206349206349]\n",
    "Accuracy  = [0.8144628099173554  ,0.8203422053231939, 0.8038277511961722 ,0.7821782178217822 ,0.6257438432665019]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(x,precision, label='Precision')\n",
    "plt.plot(x,Recall, label='Recall')\n",
    "plt.plot(x,Accuracy, label='Accuracy')\n",
    "\n",
    "plt.xlabel('file attack')\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "\n",
    "plt.legend()\n",
    "plt.title('SVM physical')\n",
    "\n",
    "plt.savefig('physical_precision_recall_accuracy.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
